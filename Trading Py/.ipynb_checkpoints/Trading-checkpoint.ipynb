{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2225f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Sukhwinder Ahluwalia\n",
    "# Purpose: Data download + analysis\n",
    "# Date: 3rd September 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d4e5d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installing necessary packages\n",
    "# !pip install pyautogui\n",
    "# !pip install time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3b6e15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import pyautogui\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac81f49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To do analyses\n",
    "# New datasets - esp non-US: pairs and commodities:  create derived datasets and opps\n",
    "# Recent surprise, quick jumps - a glut or a squeeze which may see retracements\n",
    "# Above, shorter term analysis eg Open/close gap analyses\n",
    "# New: #Open and Close seem to be nearer, High/Low are wilder. Anything there?\n",
    "\n",
    "# MA crosses and slopes not used RN\n",
    "# 'MA_3_12', 'MA_3_AT','MA_6_12', \n",
    "# ,'12m_slope','AT_slope'\n",
    "\n",
    "# #Additional filters: Volumes analysis ONLY FOR USD ONES\n",
    "# Enhancement\tDescription\tImplementation Strategy\n",
    "# Volume Validation\tIncorporate trading volume as a critical confirmation signal\t- Add volume threshold criteria for each trigger (e.g., > 1.5x average daily volume)\n",
    "# Volume Divergence\tIdentify discrepancies between price movement and volume\t- Compare current volume to historical volume patterns\n",
    "# Volume Profile\tAnalyze volume distribution at key price levels\t- Highlight high-volume nodes as potential support/resistance zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7d6d263",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download other Forex and Metals tickers\n",
    "Curr = [\n",
    "    #Currencies\n",
    "#     \"AUDCAD\", \"AUDCHF\",\n",
    "#     \"AUDJPY\", \"AUDNZD\", \"AUDSGD\", \n",
    "#     \"CADCHF\", \"CADJPY\", \"CHFJPY\", \"CHFSGD\",\n",
    "#     \"EURAUD\", \"EURCAD\", \"EURCHF\", \"EURCZK\", \"EURGBP\", \"EURHUF\", \"EURJPY\",\n",
    "#     \"EURMXN\", \"EURNOK\", \"EURNZD\", \"EURPLN\", \"EURSEK\", \"EURSGD\", \"EURTRY\", \"EURZAR\",\n",
    "#     \"GBPAUD\", \"GBPCAD\", \"GBPCHF\", \"GBPJPY\", \"GBPMXN\", \"GBPNOK\", \"GBPNZD\", \"GBPSEK\", \"GBPSGD\", \"GBPTRY\", \n",
    "#     \"NOKJPY\", \"NOKSEK\", \"NZDCAD\", \"NZDCHF\", \"NZDJPY\", \"SEKJPY\", \"SGDJPY\",\n",
    "    \"USDCAD\", \"USDCHF\", \"USDCNH\", \"USDCZK\", \"USDHUF\", \"USDJPY\", \"USDMXN\", \"USDNOK\", \"USDPLN\", \"USDSEK\",\n",
    "    \"USDSGD\", \"USDTHB\", \"USDTRY\", \"USDZAR\", \"USDIDR\", \"USDINR\", \n",
    "    'USDX.a','EURX',\n",
    "    \"EURUSD\", \"GBPUSD\",\"NZDUSD\", \"AUDUSD\", \n",
    "    \n",
    "#     #Metals\n",
    "#     \"XAGAUD\", \"XAGEUR\",  \"XAUAUD\", \"XAUCHF\", \"XAUEUR\",\n",
    "#     \"XAUGBP\", \"XAUJPY\", \n",
    "    \"XAGUSD.a\",\"XAUUSD.a\", \"XPTUSD.a\",\n",
    "    \n",
    "#     #ETFs\n",
    "    'AUS200.a','US30.a','US500.a','UK100.a','NAS100.a','EUSTX50.a','SPA35.a','JPN225.a', 'GER40.a','HK50.a',\n",
    "    \n",
    "#     # US Shares tickers\n",
    "    \"AMD.US-24\", \"BABA.US-24\", \"GOOG.US-24\", \"AMZN.US-24\", \"AAPL.US-24\", \"BAC.US-24\", \"CAT.US-24\",\n",
    "    \"CVX.US-24\", \"C.US-24\", \"XOM.US-24\", \"F.US-24\", \"GM.US-24\", \"HPQ.US-24\", \"IBM.US-24\", \"INTC.US-24\",\n",
    "    \"JPM.US-24\", \"JNJ.US-24\", \"MCD.US-24\", \"META.US-24\", \"MSFT.US-24\", \"NKE.US-24\", \"NVDA.US-24\",\n",
    "    \"NFLX.US-24\", \"ORCL.US-24\", \"PFE.US-24\", \"PG.US-24\", \"SLB.US-24\", \"SNAP.US-24\", \"TSLA.US-24\",\n",
    "    \"BA.US-24\", \"KO.US-24\", \"DIS.US-24\", \"UNH.US-24\", \"VZ.US-24\", \"RTX.US-24\", \"V.US-24\", \"WMT.US-24\"\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2290a833",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating new dfs for sans-USD\n",
    "def df_div(symbols):\n",
    "    #Step 1: Create and save the data\n",
    "    dfs = {}\n",
    "    base_path = r\"C:\\Users\\nitis\\Documents\\Forex\\Data\\New data\"\n",
    "    for sym in symbols:\n",
    "        path = fr\"{base_path}\\{sym}.csv\"\n",
    "        df = pd.read_csv(path, sep='\\t')\n",
    "        # ensure required columns\n",
    "        required = [\"<DATE>\",\"<OPEN>\",\"<HIGH>\",\"<LOW>\",\"<CLOSE>\"]\n",
    "        # normalize\n",
    "        df = df.copy()\n",
    "        df[\"<DATE>\"] = pd.to_datetime(df[\"<DATE>\"])\n",
    "        df = df.loc[:, required]  # keep exact column order\n",
    "        dfs[sym] = df\n",
    "\n",
    "    # Step 2: divide on intersection of dates to form crosses\n",
    "    cols = [\"<OPEN>\",\"<HIGH>\",\"<LOW>\",\"<CLOSE>\"]\n",
    "    result = {}  # e.g., result[\"EURGBP\"] -> DataFrame\n",
    "\n",
    "    #Loop through all datasets\n",
    "    for a, b in itertools.combinations(symbols, 2):\n",
    "        L = dfs[a]\n",
    "        R = dfs[b]\n",
    "        # ensure index is datetime and named \"<DATE>\"\n",
    "        for df in (L, R):\n",
    "            if not pd.api.types.is_datetime64_any_dtype(df.index):\n",
    "                # if <DATE> is a column, set it as index\n",
    "                if \"<DATE>\" in df.columns:\n",
    "                    df[\"<DATE>\"] = pd.to_datetime(df[\"<DATE>\"])\n",
    "                    df.set_index(\"<DATE>\", inplace=True)\n",
    "                else:\n",
    "                    raise RuntimeError(\"No datetime index or <DATE> column found in L/R\")\n",
    "\n",
    "        # now L and R have proper datetime index; do union/reindex then divide\n",
    "        common_index = L.index.union(R.index)\n",
    "        L = L.reindex(common_index)\n",
    "        R = R.reindex(common_index)\n",
    "        cross_df = L[cols].divide(R[cols])\n",
    "\n",
    "        # reset index to get <DATE> column back\n",
    "        cross_df = cross_df.reset_index().rename(columns={cross_df.columns[0]: \"<DATE>\"})\n",
    "\n",
    "        # perform division while keeping the index\n",
    "        cross_df = L[cols].divide(R[cols])\n",
    "\n",
    "        # now reset index and ensure the column is named \"<DATE>\"\n",
    "        cross_df = cross_df.reset_index()               # index -> first column\n",
    "        if cross_df.columns[0] != \"<DATE>\":\n",
    "            cross_df = cross_df.rename(columns={cross_df.columns[0]: \"<DATE>\"})\n",
    "\n",
    "        # now you can safely select ordered columns\n",
    "        cross_df = cross_df.loc[:, [\"<DATE>\"] + cols]\n",
    "\n",
    "        base = a.replace(\"USD\", \"\")\n",
    "        quote = b.replace(\"USD\", \"\")\n",
    "        cross_name = f\"{base}{quote}\"\n",
    "\n",
    "        # reset <DATE> as column and keep exact column order\n",
    "    #     cross_df = cross_df.reset_index().rename_axis(None)\n",
    "    #     cross_df = cross_df[[\"<DATE>\"] + cols]\n",
    "\n",
    "        result[cross_name] = cross_df\n",
    "\n",
    "    # print(result.keys())\n",
    "    # print(result['EURAUD'])\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d45b021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['CADCHF', 'CADCNH', 'CADCZK', 'CADHUF', 'CADJPY', 'CADMXN', 'CADNOK', 'CADPLN', 'CADSEK', 'CADSGD', 'CADTHB', 'CADTRY', 'CADZAR', 'CADIDR', 'CADINR', 'CHFCNH', 'CHFCZK', 'CHFHUF', 'CHFJPY', 'CHFMXN', 'CHFNOK', 'CHFPLN', 'CHFSEK', 'CHFSGD', 'CHFTHB', 'CHFTRY', 'CHFZAR', 'CHFIDR', 'CHFINR', 'CNHCZK', 'CNHHUF', 'CNHJPY', 'CNHMXN', 'CNHNOK', 'CNHPLN', 'CNHSEK', 'CNHSGD', 'CNHTHB', 'CNHTRY', 'CNHZAR', 'CNHIDR', 'CNHINR', 'CZKHUF', 'CZKJPY', 'CZKMXN', 'CZKNOK', 'CZKPLN', 'CZKSEK', 'CZKSGD', 'CZKTHB', 'CZKTRY', 'CZKZAR', 'CZKIDR', 'CZKINR', 'HUFJPY', 'HUFMXN', 'HUFNOK', 'HUFPLN', 'HUFSEK', 'HUFSGD', 'HUFTHB', 'HUFTRY', 'HUFZAR', 'HUFIDR', 'HUFINR', 'JPYMXN', 'JPYNOK', 'JPYPLN', 'JPYSEK', 'JPYSGD', 'JPYTHB', 'JPYTRY', 'JPYZAR', 'JPYIDR', 'JPYINR', 'MXNNOK', 'MXNPLN', 'MXNSEK', 'MXNSGD', 'MXNTHB', 'MXNTRY', 'MXNZAR', 'MXNIDR', 'MXNINR', 'NOKPLN', 'NOKSEK', 'NOKSGD', 'NOKTHB', 'NOKTRY', 'NOKZAR', 'NOKIDR', 'NOKINR', 'PLNSEK', 'PLNSGD', 'PLNTHB', 'PLNTRY', 'PLNZAR', 'PLNIDR', 'PLNINR', 'SEKSGD', 'SEKTHB', 'SEKTRY', 'SEKZAR', 'SEKIDR', 'SEKINR', 'SGDTHB', 'SGDTRY', 'SGDZAR', 'SGDIDR', 'SGDINR', 'THBTRY', 'THBZAR', 'THBIDR', 'THBINR', 'TRYZAR', 'TRYIDR', 'TRYINR', 'ZARIDR', 'ZARINR', 'IDRINR'])\n"
     ]
    }
   ],
   "source": [
    "#Creating new dfs for sans-USD\n",
    "#One-way symbols:\n",
    "symbols = [\"EURUSD\", \"GBPUSD\", \"AUDUSD\", \"NZDUSD\"]\n",
    "res1 = df_div(symbols)\n",
    "# print(res1['EURAUD'])\n",
    "\n",
    "#Other-way symbols: \n",
    "Curr2 = [\"USDCAD\", \"USDCHF\", \"USDCNH\", \"USDCZK\", \"USDHUF\", \"USDJPY\", \"USDMXN\", \"USDNOK\", \"USDPLN\", \"USDSEK\",\n",
    "    \"USDSGD\", \"USDTHB\", \"USDTRY\", \"USDZAR\", \"USDIDR\", \"USDINR\"]\n",
    "res2 = df_div(Curr2)\n",
    "print(res2.keys())\n",
    "# print(res2['CHFCAD'])\n",
    "\n",
    "# Do for others and then crosses by using a function for the below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1ef235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! python3\n",
    "# mouseNow.py - Displays the mouse cursor's current position.\n",
    "#Mouse coordinates. = x = (0,1919) y = (0,1079)\n",
    "\n",
    "pyautogui.PAUSE = 4\n",
    "\n",
    "#Get to Pepperstone app\n",
    "pyautogui.click(25, 900)\n",
    "# pyautogui.PAUSE = 1\n",
    "pyautogui.click(350, 200)\n",
    "#pyautogui.hotkey('alt', 'tab')\n",
    "pyautogui.PAUSE = 2\n",
    "\n",
    "#Get to currency page\n",
    "pyautogui.hotkey('ctrl', 'u')\n",
    "\n",
    "#Get to Bars\n",
    "pyautogui.click(500, 235)\n",
    "#Download AUDUSD\n",
    "pyautogui.typewrite(\"audusd\")\n",
    "pyautogui.hotkey('enter')\n",
    "pyautogui.hotkey('tab')\n",
    "pyautogui.typewrite('d') #Daily\n",
    "pyautogui.click(1000, 265) #Request\n",
    "pyautogui.click(480, 640) #Export\n",
    "#time.sleep(2)\n",
    "pyautogui.click(700, 215) #Change the directory for these\n",
    "pyautogui.typewrite(\"C:\\\\Users\\\\nitis\\\\Documents\\\\Forex\\\\Data\\\\New data\") #Use directory\n",
    "pyautogui.hotkey('enter')\n",
    "pyautogui.click(700, 550) #Change the name\n",
    "pyautogui.typewrite('audusd.csv') #Save file as aud\n",
    "pyautogui.hotkey('enter')\n",
    "pyautogui.typewrite('y')\n",
    "\n",
    "#Loop through all currencies, len(daily)\n",
    "for i in range(0,len(Curr)):\n",
    "    pyautogui.click(490, 265) #Currency type\n",
    "    pyautogui.typewrite(Curr[i])\n",
    "    pyautogui.hotkey('enter')\n",
    "    pyautogui.click(1000, 265) #Request\n",
    "    pyautogui.click(1000, 265) #Request again\n",
    "    pyautogui.click(480, 640) #Export\n",
    "#    time.sleep(2)\n",
    "    nm = fr\"{Curr[i]}.csv\" #Name of file\n",
    "    pyautogui.typewrite(nm) #Save file \n",
    "    pyautogui.hotkey('enter')\n",
    "    pyautogui.typewrite('y')\n",
    "\n",
    "#Exit currency page\n",
    "pyautogui.hotkey('esc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68ab67e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data and basic analyses\n",
    "#Get slope for the MAs\n",
    "def numpy_slope(series, window=10):\n",
    "    def calc_slope(x):\n",
    "        if len(x) < window:\n",
    "            return np.nan\n",
    "        coeffs = np.polyfit(range(len(x)), x, 1)\n",
    "        return coeffs[0]\n",
    "    \n",
    "    return series.rolling(window=window).apply(calc_slope)\n",
    "\n",
    "#Run function\n",
    "def Run(df,name):\n",
    "    #Currency and dates\n",
    "    df['Currency'] = name\n",
    "    df['<DATE>'] = pd.to_datetime(df['<DATE>'], format='%Y.%m.%d')\n",
    "    df.set_index('<DATE>')\n",
    "    # Note these below apply to Lows and Highs\n",
    "    df['1m_MA_Filter'] = 0 \n",
    "    df['3m_MA_Filter'] = 0\n",
    "    df['6m_MA_Filter'] = 0\n",
    "    df['12m_MA_Filter'] = 0\n",
    "    df['AT_MA_Filter'] = 0\n",
    "\n",
    "    #High/Low since 1 month: 23 days or so\n",
    "    df['1m_High'] = df['<CLOSE>'].rolling(window=23).max()\n",
    "    df['1m_Low'] = df['<CLOSE>'].rolling(window=23).min()\n",
    "    df['1m_Mean'] = df['<CLOSE>'].rolling(23).mean()\n",
    "        \n",
    "    #High/Low since 3 months: 70 days or so\n",
    "    df['3m_High'] = df['<CLOSE>'].rolling(window=70).max()\n",
    "    df['3m_Low'] = df['<CLOSE>'].rolling(window=70).min()\n",
    "    df['3m_Mean'] = df['<CLOSE>'].rolling(70).mean()\n",
    "    \n",
    "    #High/Low since 6 months: 135 days or so\n",
    "    df['6m_High'] = df['<CLOSE>'].rolling(window=135).max()\n",
    "    df['6m_Low'] = df['<CLOSE>'].rolling(window=135).min()\n",
    "    df['6m_Mean'] = df['<CLOSE>'].rolling(135).mean()\n",
    "    \n",
    "    #High/Low since 12 months: 260 days or so\n",
    "    df['12m_High'] = df['<CLOSE>'].rolling(window=260).max()\n",
    "    df['12m_Low'] = df['<CLOSE>'].rolling(window=260).min()\n",
    "    df['12m_Mean'] = df['<CLOSE>'].rolling(260).mean()\n",
    "        \n",
    "    #High/Low since 2022\n",
    "    df['AT_High'] = df['<CLOSE>'].max()\n",
    "    df['AT_Low'] = df['<CLOSE>'].min()\n",
    "    df['AT_Mean'] = df['<CLOSE>'].mean()\n",
    "    df['AT_Med'] = df['<CLOSE>'].median()\n",
    "    \n",
    "    # List of time periods\n",
    "    time_periods = ['1m', '3m', '6m', '12m', 'AT']\n",
    "\n",
    "    # Create 90th percentile slope filters for each time period\n",
    "    for period in time_periods:\n",
    "        # Create MA Filter for High and Close\n",
    "        df.loc[round(df[f'{period}_High'], 2) == round(df['<CLOSE>'], 2), f'{period}_MA_Filter'] = 1\n",
    "\n",
    "        # Create MA Filter for Low and Close\n",
    "        df.loc[round(df[f'{period}_Low'], 2) == round(df['<CLOSE>'], 2), f'{period}_MA_Filter'] = 1\n",
    "\n",
    "        # Calculate Slope\n",
    "        df[f'{period}_MA_slope'] = numpy_slope(df[f'{period}_Mean'])\n",
    "\n",
    "        # Normalize Slope\n",
    "        df[f'{period}_MA_slope_nm'] = (df[f'{period}_MA_slope'] - df[f'{period}_MA_slope'].mean()) / df[f'{period}_MA_slope'].std()\n",
    "       \n",
    "        # Create binary filter column on the percentile\n",
    "        df[f'{period}_slope'] = (df[f'{period}_MA_slope_nm'] >= df[f'{period}_MA_slope_nm'].quantile(0.9)).astype(int)\n",
    "    \n",
    "    # MA Crosses: Where rounded till 3 is the same eg 0.6659 = 0.6663 = 0.666 etc\n",
    "    # 1m vs 3m\n",
    "    df.loc[round(df['1m_Mean'], 3) == round(df['3m_Mean'], 3), 'MA_1_3'] = 1\n",
    "\n",
    "    # 1m vs 6m\n",
    "    df.loc[round(df['1m_Mean'], 3) == round(df['6m_Mean'], 3), 'MA_1_6'] = 1\n",
    "\n",
    "    # 1m vs 12m\n",
    "    df.loc[round(df['1m_Mean'], 3) == round(df['12m_Mean'], 3), 'MA_1_12'] = 1\n",
    "\n",
    "    # 1m vs AT\n",
    "    df.loc[round(df['1m_Mean'], 3) == round(df['AT_Mean'], 3), 'MA_1_AT'] = 1\n",
    "\n",
    "    # 3m vs 6m\n",
    "    df.loc[round(df['3m_Mean'], 3) == round(df['6m_Mean'], 3), 'MA_3_6'] = 1\n",
    "\n",
    "    # 3m vs 12m\n",
    "    df.loc[round(df['3m_Mean'], 3) == round(df['12m_Mean'], 3), 'MA_3_12'] = 1\n",
    "\n",
    "    # 3m vs AT\n",
    "    df.loc[round(df['3m_Mean'], 3) == round(df['AT_Mean'], 3), 'MA_3_AT'] = 1\n",
    "\n",
    "    # 6m vs 12m\n",
    "    df.loc[round(df['6m_Mean'], 3) == round(df['12m_Mean'], 3), 'MA_6_12'] = 1\n",
    "\n",
    "    # 6m vs AT\n",
    "    df.loc[round(df['6m_Mean'], 3) == round(df['AT_Mean'], 3), 'MA_6_AT'] = 1\n",
    "\n",
    "    # 12m vs AT\n",
    "    df.loc[round(df['12m_Mean'], 3) == round(df['AT_Mean'], 3), 'MA_12_AT'] = 1\n",
    "\n",
    "    # Optional: Fill NaN with 0 if needed\n",
    "    ma_filter_columns = [\n",
    "        'MA_1_3', 'MA_1_6', 'MA_1_12', 'MA_1_AT', \n",
    "        'MA_3_6', 'MA_3_12', 'MA_3_AT', \n",
    "        'MA_6_12', 'MA_6_AT', \n",
    "        'MA_12_AT'\n",
    "    ]\n",
    "    \n",
    "    for col in ma_filter_columns:\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "653661fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trends. Where opps lie:\n",
    "def Filter(df_c, filter_columns):\n",
    "    # Create the filter condition\n",
    "    filter_condition = ' | '.join([f'(df_c[\"{col}\"]==1)' for col in filter_columns])\n",
    "    \n",
    "    # Create cumulative filter column\n",
    "    df_c2 = df_c.copy()\n",
    "    df_c2['Total_Filters'] = df_c2[filter_columns].sum(axis=1)\n",
    "    \n",
    "    # Apply the filter\n",
    "    return df_c2[eval(filter_condition)]\n",
    "\n",
    "#Checks\n",
    "# df_wk['12m_MA_Filter'].value_counts()\n",
    "# df_st['12m_MA_Filter'].value_counts()\n",
    "# df_wk.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83eb7283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Currency  St_Filters  Wk_Filters\n",
      "0     XAGUSD.a         2.0         6.0\n",
      "1   GOOG.US-24         2.0         6.0\n",
      "2       USDINR         2.0         5.0\n",
      "3     XAUUSD.a         2.0         5.0\n",
      "4    BAC.US-24         2.0         5.0\n",
      "5   ORCL.US-24         2.0         5.0\n",
      "6       EURUSD         2.0         4.0\n",
      "7      US500.a         2.0         4.0\n",
      "8     JPN225.a         2.0         4.0\n",
      "9   BABA.US-24         2.0         4.0\n",
      "10     C.US-24         2.0         4.0\n",
      "11   JPM.US-24         2.0         4.0\n",
      "12      USDCHF         2.0         3.0\n",
      "13      USDCZK         2.0         3.0\n",
      "14      USDTHB         2.0         3.0\n",
      "15      USDTRY         2.0         3.0\n",
      "16      US30.a         2.0         3.0\n",
      "17    NAS100.a         2.0         3.0\n",
      "18      HK50.a         2.0         3.0\n",
      "19      USDHUF         1.0         3.0\n",
      "20      USDMXN         1.0         3.0\n",
      "21      USDNOK         1.0         3.0\n",
      "22      USDSEK         1.0         3.0\n"
     ]
    }
   ],
   "source": [
    "# Analysis on the datasets\n",
    "base_path = r\"C:\\Users\\nitis\\Documents\\Forex\\Data\\New data\"\n",
    "#  store non-empty dfs\n",
    "collected_st = []\n",
    "collected_wk = []\n",
    "\n",
    "#Loop through all currencies, len(daily)\n",
    "for i in range(0,len(Curr)):\n",
    "    path = fr\"{base_path}\\{Curr[i]}.csv\"\n",
    "    df = pd.read_csv(path, sep='\\t', engine='python')\n",
    "    df2 = Run(df,Curr[i])\n",
    "    df2 = df2.tail()\n",
    "\n",
    "    # Strong set: 4 filters\n",
    "    # - Close is near AT and/or 12m H/L then time could be to reverse - Strong\n",
    "    # - Rolling MA crosses comparisons for 12 with various and AT\n",
    "    df_st = Filter(df2, ['AT_MA_Filter', '12m_MA_Filter'\n",
    "                        ,'MA_6_AT','MA_12_AT'])\n",
    "    \n",
    "    df_st = df_st [['Currency', 'Total_Filters']].sort_values(by='Total_Filters', ascending=False).reset_index(drop=True)\n",
    "    df_st = df_st.drop_duplicates(subset=['Currency']) #, 'Total_Filters'\n",
    "    \n",
    "    # Weak set: 11 filters\n",
    "    # - Close is near other H/L then time could be to reverse - Weak\n",
    "    # - Rolling MA crosses comparisons for 1 with various and 3/6\n",
    "    # - Slopes for sudden jumps\n",
    "    df_wk = Filter(df2, ['1m_MA_Filter','3m_MA_Filter','6m_MA_Filter'\n",
    "                         ,'MA_1_3', 'MA_1_6', 'MA_1_12', 'MA_1_AT','MA_3_6'\n",
    "                        ,'1m_slope','3m_slope','6m_slope'])\n",
    "    \n",
    "    df_wk = df_wk [['Currency', 'Total_Filters']].sort_values(by='Total_Filters', ascending=False).reset_index(drop=True)   \n",
    "    df_wk = df_wk.drop_duplicates(subset=['Currency'])\n",
    "\n",
    "    #Collect data in one set\n",
    "    if not df_st.empty:\n",
    "        collected_st.append(df_st)\n",
    "\n",
    "    if not df_wk.empty:\n",
    "        collected_wk.append(df_wk)\n",
    "\n",
    "if collected_st:\n",
    "    result_st = pd.concat( collected_st, ignore_index=True)\n",
    "else:\n",
    "    result_st = pd.DataFrame()  # empty result\n",
    "\n",
    "if collected_wk:\n",
    "    result_wk = pd.concat(collected_wk, ignore_index=True)\n",
    "else:\n",
    "    result_wk = pd.DataFrame()  # empty result\n",
    "\n",
    "#Final df\n",
    "df_fin = result_st.merge(result_wk, on='Currency', how='left')\n",
    "df_fin.rename(columns={'Total_Filters_x': 'St_Filters','Total_Filters_y': 'Wk_Filters'}, inplace=True)\n",
    "df_fin = df_fin.sort_values(by=['St_Filters','Wk_Filters'], ascending=False).reset_index(drop=True)   \n",
    "print(df_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa12545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appendix below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d90d744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Other shares, softs, hards, etc\n",
    "\n",
    "# # AU Shares tickers\n",
    "# au_share_tickers = [\n",
    "#     \"A2M.AU\", \"ABC.AU\", \"AGL.AU\", \"AIA.AU\", \"AIZ.AU\", \"ALD.AU\", \"ALQ.AU\", \"ALU.AU\", \"ALX.AU\", \"AMC.AU\",\n",
    "#     \"AMP.AU\", \"ANN.AU\", \"ANZ.AU\", \"APA.AU\", \"APE.AU\", \"APX.AU\", \"ARG.AU\", \"ARB.AU\", \"ASB.AU\", \"ASX.AU\",\n",
    "#     \"AST.AU\", \"AUB.AU\", \"AWB.AU\", \"AWL.AU\", \"AWN.AU\", \"AWC.AU\", \"BEN.AU\", \"BGA.AU\", \"BHP.AU\", \"BIN.AU\",\n",
    "#     \"BKL.AU\", \"BKW.AU\", \"BLD.AU\", \"BOQ.AU\", \"BPT.AU\", \"BRG.AU\", \"BSL.AU\", \"BVS.AU\", \"BWP.AU\", \"BXB.AU\",\n",
    "#     \"CAR.AU\", \"CBA.AU\", \"CCP.AU\", \"CCL.AU\", \"CGC.AU\", \"CGF.AU\", \"CHC.AU\", \"CIP.AU\", \"CIM.AU\", \"CIA.AU\",\n",
    "#     \"CLW.AU\", \"CMW.AU\", \"CNU.AU\", \"COL.AU\", \"COE.AU\", \"COH.AU\", \"CPU.AU\", \"CQR.AU\", \"CRN.AU\", \"CSL.AU\",\n",
    "#     \"CSR.AU\", \"CTD.AU\", \"CUV.AU\", \"CWN.AU\", \"CWY.AU\", \"DHG.AU\", \"DMP.AU\", \"DOW.AU\", \"DDR.AU\", \"DEG.AU\",\n",
    "#     \"DRR.AU\", \"DXS.AU\", \"EBO.AU\", \"EHE.AU\", \"EHL.AU\", \"ELD.AU\", \"EHE.AU\", \"EML.AU\", \"EVT.AU\", \"EVN.AU\",\n",
    "#     \"FBU.AU\", \"FLT.AU\", \"FMG.AU\", \"FPH.AU\", \"GEM.AU\", \"GNC.AU\", \"GMG.AU\", \"GOR.AU\", \"GOZ.AU\", \"GPT.AU\",\n",
    "#     \"GQG.AU\", \"GUD.AU\", \"GWA.AU\", \"HDN.AU\", \"HLS.AU\", \"HMC.AU\", \"HUB.AU\", \"HVN.AU\", \"IAG.AU\", \"IFT.AU\",\n",
    "#     \"IEL.AU\", \"IFM.AU\", \"IFL.AU\", \"ILU.AU\", \"INA.AU\", \"INC.AU\", \"IRE.AU\", \"IPL.AU\", \"IPH.AU\", \"IVC.AU\",\n",
    "#     \"JBH.AU\", \"JHX.AU\", \"JHG.AU\", \"JIN.AU\", \"KGN.AU\", \"LNK.AU\", \"LIS.AU\", \"LLC.AU\", \"LOV.AU\", \"LTR.AU\",\n",
    "#     \"LYC.AU\", \"MFG.AU\", \"MIN.AU\", \"MGR.AU\", \"MMS.AU\", \"MPL.AU\", \"MP1.AU\", \"MQG.AU\", \"MTS.AU\", \"MYX.AU\",\n",
    "#     \"NAN.AU\", \"NAB.AU\", \"NEC.AU\", \"NEU.AU\", \"NHC.AU\", \"NHF.AU\", \"NIC.AU\", \"NEM.AU\", \"NCM.AU\", \"NEA.AU\",\n",
    "#     \"NUF.AU\", \"NSR.AU\", \"NWH.AU\", \"NXT.AU\", \"NWL.AU\", \"ORA.AU\", \"ORG.AU\", \"ORI.AU\", \"OML.AU\", \"ORE.AU\",\n",
    "#     \"OZL.AU\", \"PDL.AU\", \"PDN.AU\", \"PER.AU\", \"PLS.AU\", \"PME.AU\", \"PMV.AU\", \"PNI.AU\", \"PNV.AU\", \"PPK.AU\",\n",
    "#     \"PPT.AU\", \"PRN.AU\", \"PRU.AU\", \"PTM.AU\", \"PXA.AU\", \"QAN.AU\", \"QBE.AU\", \"QUB.AU\", \"RBL.AU\", \"REA.AU\",\n",
    "#     \"REH.AU\", \"REG.AU\", \"RHC.AU\", \"RIO.AU\", \"RMD.AU\", \"RRL.AU\", \"RSG.AU\", \"RWC.AU\", \"S32.AU\", \"SAR.AU\",\n",
    "#     \"SBM.AU\", \"SCA.AU\", \"SCG.AU\", \"SCP.AU\", \"SDG.AU\", \"SDF.AU\", \"SEK.AU\", \"SFR.AU\", \"SGM.AU\", \"SHL.AU\",\n",
    "#     \"SIQ.AU\", \"SKC.AU\", \"SKI.AU\", \"SLC.AU\", \"SLR.AU\", \"SML.AU\", \"SMR.AU\", \"SOL.AU\", \"SPK.AU\", \"SSG.AU\",\n",
    "#     \"STO.AU\", \"SUL.AU\", \"SUN.AU\", \"SVW.AU\", \"SWM.AU\", \"SYD.AU\", \"SXL.AU\", \"TAH.AU\", \"TCL.AU\", \"TGR.AU\",\n",
    "#     \"TLS.AU\", \"TLX.AU\", \"TNE.AU\", \"TPG.AU\", \"TWE.AU\", \"URW.AU\", \"VCX.AU\", \"VEA.AU\", \"VOC.AU\", \"VNT.AU\",\n",
    "#     \"VUK.AU\", \"WBC.AU\", \"WEB.AU\", \"WES.AU\", \"WMC.AU\", \"WOR.AU\", \"WOW.AU\", \"WPL.AU\", \"WPR.AU\", \"WSA.AU\",\n",
    "#     \"WTC.AU\", \"XRO.AU\", \"YAL.AU\", \"Z1P.AU\"\n",
    "# ]\n",
    "\n",
    "# # Combine all tickers and sort\n",
    "# all_tickers = sorted(au_share_tickers + us_share_tickers + forex_metals_tickers)\n",
    "\n",
    "# # Create DataFrame and export to CSV\n",
    "# df = pd.DataFrame(all_tickers, columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9323fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing here again JIC\n",
    "# #! python3\n",
    "# # mouseNow.py - Displays the mouse cursor's current position.\n",
    "# import pyautogui\n",
    "# import time\n",
    "# #Mouse coordinates. = x = (0,1919) y = (0,1079)\n",
    "\n",
    "# pyautogui.PAUSE = 4\n",
    "\n",
    "# #Get to Pepperstone app\n",
    "# pyautogui.click(25, 900)\n",
    "# # pyautogui.PAUSE = 1\n",
    "# pyautogui.click(350, 200)\n",
    "# #pyautogui.hotkey('alt', 'tab')\n",
    "# pyautogui.PAUSE = 2\n",
    "\n",
    "# #Get to currency page\n",
    "# pyautogui.hotkey('ctrl', 'u')\n",
    "\n",
    "# #Get to Bars\n",
    "# pyautogui.click(500, 235)\n",
    "# #Download AUDUSD\n",
    "# pyautogui.typewrite(\"audusd\")\n",
    "# pyautogui.hotkey('enter')\n",
    "# pyautogui.hotkey('tab')\n",
    "# pyautogui.typewrite('d') #Daily\n",
    "# pyautogui.click(1000, 265) #Request\n",
    "# pyautogui.click(480, 640) #Export\n",
    "# #time.sleep(2)\n",
    "# pyautogui.click(700, 215) #Change the directory for these\n",
    "# pyautogui.typewrite(\"C:\\\\Users\\\\nitis\\\\Documents\\\\Forex\\\\Data\\\\New data\") #Use directory\n",
    "# pyautogui.hotkey('enter')\n",
    "# pyautogui.click(700, 550) #Change the name\n",
    "# pyautogui.typewrite('audusd') #Save file as aud\n",
    "# pyautogui.hotkey('enter')\n",
    "# pyautogui.typewrite('y')\n",
    "\n",
    "# #Loop through all currencies, len(daily)\n",
    "# for i in range(0,len(Curr)):\n",
    "#     pyautogui.click(490, 265) #Currency type\n",
    "#     pyautogui.typewrite(Curr[i])\n",
    "#     pyautogui.hotkey('enter')\n",
    "#     pyautogui.click(1000, 265) #Request\n",
    "#     pyautogui.click(480, 640) #Export\n",
    "# #    time.sleep(2)\n",
    "#     pyautogui.typewrite(Curr[i]) #Save file \n",
    "#     pyautogui.hotkey('enter')\n",
    "#     pyautogui.typewrite('y')\n",
    "\n",
    "# #Exit currency page\n",
    "# pyautogui.hotkey('esc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7ea375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check on how to split data better\n",
    "# base_path_hist = r\"C:\\Users\\nitis\\Documents\\Forex\\Data\\New data\\Hist data\"\n",
    "# #  store non-empty dfs\n",
    "# collected_st = []\n",
    "# collected_wk = []\n",
    "\n",
    "# from io import StringIO\n",
    "\n",
    "# #Loop through all currencies, len(daily)\n",
    "# for i in range(0,len(Curr)):\n",
    "#     path = fr\"{base_path_hist}\\{Curr[i]}.csv\"    \n",
    "#     with open(path, 'r', encoding='utf-8') as f:\n",
    "#         text = f.read()\n",
    "#     # remove one pair of surrounding quotes if present\n",
    "#     if text.startswith('\"') and text.count('\\n')>0:\n",
    "#         text = text.lstrip('\"').rstrip('\\n')\n",
    "#         # also remove a trailing closing quote on the header line if present\n",
    "#         first_line, rest = text.split('\\n', 1)\n",
    "#         if first_line.endswith('\"'):\n",
    "#             first_line = first_line.rstrip('\"')\n",
    "#         text = first_line + '\\n' + rest\n",
    "#     df = pd.read_csv(StringIO(text))\n",
    "#     print(df)\n",
    "\n",
    "# #     with open(path, 'rb') as f:\n",
    "# #         first = f.readline()\n",
    "# #         print(first)          # shows raw bytes; look for b'\\\\t' vs b'\\t'\n",
    "# #         print(first.decode('utf-8'))  # shows literal \\t if present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ed27d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Join on to historical datasets\n",
    "# # Analysis on the datasets\n",
    "# base_path = r\"C:\\Users\\nitis\\Documents\\Forex\\Data\\New data\"\n",
    "# base_path_hist = r\"C:\\Users\\nitis\\Documents\\Forex\\Data\\New data\\Hist data\"\n",
    "\n",
    "# #Loop through all currencies, len(daily)\n",
    "# for i in range(0,len(Curr)):\n",
    "#     #Load new data\n",
    "#     path = fr\"{base_path}\\{Curr[i]}.csv\"\n",
    "#     df = pd.read_csv(path, sep='\\t')\n",
    "#     #Load hist data\n",
    "#     path_hist = fr\"{base_path_hist}\\{Curr[i]}.csv\"\n",
    "#     df_hist = pd.read_csv(path_hist, sep='\\t')\n",
    "#     #Save total data in hist\n",
    "#     combined = pd.concat([df_hist, df], ignore_index=True)\n",
    "#     combined = combined.drop_duplicates()\n",
    "#     combined.to_csv(path_hist, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb59b890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make DATE a datetime index (recommended)\n",
    "# for df in (eur_df, cad_df):\n",
    "#     df[\"<DATE>\"] = pd.to_datetime(df[\"<DATE>\"])\n",
    "#     df.set_index(\"<DATE>\", inplace=True)\n",
    "\n",
    "# # align on DATE and multiply numeric columns elementwise\n",
    "# # method: reindex union of dates then multiply (NaNs remain if a date missing in one df)\n",
    "# common_index = eur_df.index.union(cad_df.index)\n",
    "# left = eur_df.reindex(common_index)\n",
    "# right = cad_df.reindex(common_index)\n",
    "\n",
    "# # multiply elementwise for the four columns\n",
    "# cols = [\"<OPEN>\",\"<HIGH>\",\"<LOW>\",\"<CLOSE>\"] #Open and Close seem to be nearer, High/Low are wilder\n",
    "# result = left[cols].multiply(right[cols])\n",
    "\n",
    "# # optional: if you prefer to drop rows with any NaN (dates not present in both)\n",
    "# result_dropped = result.dropna(how=\"any\")\n",
    "\n",
    "# print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
